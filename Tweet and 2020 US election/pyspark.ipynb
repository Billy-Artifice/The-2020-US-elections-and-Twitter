{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "Reading hashtag_donaldtrump.csv\n",
      "type of data:  <class 'pyspark.rdd.RDD'>\n",
      "---------------------------\n"
     ]
    }
   ],
   "source": [
    "#import libraries and define functions\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as pyfunc\n",
    "from pyspark.sql.functions import udf, col, lower ,regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.feature import IDF as MLIDF\n",
    "from pyspark.ml.feature import HashingTF as MLHashingTF\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from yellowbrick.text import FreqDistVisualizer\n",
    "from yellowbrick.datasets import load_hobbies\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from joblib import Parallel\n",
    "from multiprocessing import Pool\n",
    "from dask import dataframe as ddf\n",
    "from dask.delayed import delayed\n",
    "import dask.bag as db\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "sr = stopwords.words('english')\n",
    "sp = stopwords.words('spanish')\n",
    "\n",
    "stopwordsList = []\n",
    "stopwordsList += sr\n",
    "stopwordsList += sp\n",
    "\n",
    "def cleanSentence(s):\n",
    "    s = lower(s)\n",
    "    #Retweet\n",
    "    s = regexp_replace(s, \"^rt \", \"\")\n",
    "    #hyperlink\n",
    "    s = regexp_replace(s, \"(https?\\://)\\S+\", \"\")\n",
    "    #non-eng character\n",
    "    s = regexp_replace(s, \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    "    #hashtag\n",
    "    s = regexp_replace(s, \"/#\\w+\\s*/\", \"\")\n",
    "    #meaningless number\n",
    "    s = regexp_replace(s, \"[00-99]\", \"\")\n",
    "    return s\n",
    "\n",
    "    \n",
    "sc = SparkSession.builder.appName(\"WordCount\").getOrCreate()\n",
    "sqlContext=SQLContext(sparkContext=sc.sparkContext, sparkSession=sc)\n",
    "\n",
    "print('---------------------------')\n",
    "print(sc.sparkContext)\n",
    "\n",
    "trumpRDD = sc.sparkContext.textFile('hashtag_donaldtrump.csv')\n",
    "print('Reading hashtag_donaldtrump.csv')\n",
    "print('type of data: ' , type(trumpRDD))\n",
    "print('---------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Trump Reading Time:  5.507136106491089\n",
      "+--------------------+--------------------+\n",
      "|            filtered|            features|\n",
      "+--------------------+--------------------+\n",
      "|[elecciones, , fl...|(262144,[1303,304...|\n",
      "|                  []|(262144,[249180],...|\n",
      "|                  []|(262144,[249180],...|\n",
      "|[usa, , trump, co...|(262144,[1512,595...|\n",
      "|[trump, student, ...|(262144,[55307,66...|\n",
      "|                  []|(262144,[249180],...|\n",
      "|[, hours, since, ...|(262144,[329,5381...|\n",
      "|[get, tie, get, t...|(262144,[46479,12...|\n",
      "|[clady, , minutes...|(262144,[34366,36...|\n",
      "|                  []|(262144,[249180],...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o123.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\r\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\r\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\r\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\r\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\r\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\r\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\r\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema(IDF.scala:60)\r\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema$(IDF.scala:59)\r\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:69)\r\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:99)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:89)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:69)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-16b4775b90ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0midf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0midf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    127\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    316\u001b[0m         \"\"\"\n\u001b[0;32m    317\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o123.fit.\n: java.util.NoSuchElementException: Failed to find a default value for inputCol\r\n\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\r\n\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\r\n\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\r\n\tat org.apache.spark.ml.param.Params.$(params.scala:762)\r\n\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\r\n\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\r\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema(IDF.scala:60)\r\n\tat org.apache.spark.ml.feature.IDFBase.validateAndTransformSchema$(IDF.scala:59)\r\n\tat org.apache.spark.ml.feature.IDF.validateAndTransformSchema(IDF.scala:69)\r\n\tat org.apache.spark.ml.feature.IDF.transformSchema(IDF.scala:99)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:89)\r\n\tat org.apache.spark.ml.feature.IDF.fit(IDF.scala:69)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "#handling Trump Data with PySpark\n",
    "\n",
    "readStartTime = time.time()\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('hashtag_donaldtrump.csv', header=True)\n",
    "print('PySpark Trump Reading Time: ', time.time() - readStartTime)\n",
    "#df = sc.sparkContext.textFile('hashtag_donaldtrump.csv')\n",
    "df = df.na.drop(subset=[\"tweet\"])\n",
    "\n",
    "\n",
    "#print(type(rdd))\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cleanDF = df.select(cleanSentence(col(\"tweet\")).alias(\"words\"))\n",
    "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"vector\")\n",
    "countDF = tokenizer.transform(cleanDF).select(\"vector\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"vector\", outputCol=\"filtered\", stopWords=stopwordsList)\n",
    "noStopDF = remover.transform(countDF).select(\"filtered\").where(col(\"filtered\").isNotNull())\n",
    "\n",
    "#cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=10.0)\n",
    "#model = cv.fit(noStopDF)\n",
    "#result = model.transform(noStopDF)\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "#from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "#ht = HashingTF(inputCol=\"filtered\", outputCol=\"features\")\n",
    "#result = ht.transform(noStopDF)\n",
    "\n",
    "#result.cache()\n",
    "#idf = IDF().fit(result)\n",
    "#tfidf = idf.transform(result)\n",
    "\n",
    "\n",
    "noStopDFCount = noStopDF.select(pyfunc.explode(\"filtered\").alias(\"word\"))\\\n",
    "                .groupBy(\"word\").count()\\\n",
    "                .filter(\"`word` != \\'\\'\")\\\n",
    "                .sort(col(\"count\").desc())\n",
    "\n",
    "\n",
    "print('PySpark Trump Word Count: ' ,time.time()-startTime)\n",
    "\n",
    "print('-----DataFrame Representation-----')\n",
    "noStopDFCount.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling Biden Data with PySpark\n",
    "\n",
    "startTime = time.time()\n",
    "df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('hashtag_joebiden.csv')\n",
    "print('PySpark Biden Reading time: ', time.time() - startTime)\n",
    "df = df.na.drop(subset=[\"tweet\"])\n",
    "\n",
    "\n",
    "\n",
    "cleanDF = df.select(cleanSentence(col(\"tweet\")).alias(\"words\"))\n",
    "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"vector\")\n",
    "countDF = tokenizer.transform(cleanDF).select(\"vector\")\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"vector\", outputCol=\"filtered\", stopWords=stopwordsList)\n",
    "noStopDF = remover.transform(countDF).select(\"filtered\").where(col(\"filtered\").isNotNull())\n",
    "\n",
    "#startTime = time.time()\n",
    "#cv = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", minDF=10.0)\n",
    "#model = cv.fit(noStopDF)\n",
    "#result = model.transform(noStopDF)\n",
    "#print( time.time()-startTime)\n",
    "\n",
    "startTime = time.time()\n",
    "noStopDFCount = noStopDF.select(pyfunc.explode(\"filtered\").alias(\"word\"))\\\n",
    "                .groupBy(\"word\").count()\\\n",
    "                .filter(\"`word` != \\'\\'\")\\\n",
    "                .sort(col(\"count\").desc())\n",
    "print('PySpark Biden WordCount Time: ' ,time.time()-startTime)\n",
    "noStopDFCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer as SKCountVectorizer\n",
    "\n",
    "startTime = time.time()\n",
    "df = pd.read_csv('hashtag_donaldtrump.csv', lineterminator='\\n')\n",
    "print('Pandas Trump Reading Time: ', time.time()-startTime)\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "dfCount = df.tweet.str.split(expand=True).stack().value_counts()\n",
    "\n",
    "\n",
    "print('Pandas Trump WordCount Time: ', time.time()-startTime)\n",
    "\n",
    "startTime = time.time()\n",
    "vectorizer = SKCountVectorizer(max_features=5000)\n",
    "docs = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "\n",
    "print(time.time() - startTime)\n",
    "print('end')\n",
    "#feature_names = vectorizer.get_feature_names()\n",
    "#visualizer = FreqDistVisualizer(features=feature_names, orient='v')\n",
    "#visualizer.fit(docs.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer as SKCountVectorizer\n",
    "\n",
    "startTime = time.time()\n",
    "df = pd.read_csv('hashtag_joebiden.csv', lineterminator='\\n')\n",
    "print('Pandas Biden Reading Time: ', time.time()-startTime)\n",
    "\n",
    "startTime = time.time()\n",
    "dfCount = df.tweet.str.split(expand=True).stack().value_counts()\n",
    "print('Pandas Biden WordCount Time: ', time.time()-startTime)\n",
    "\n",
    "vectorizer = SKCountVectorizer(max_features=5000)\n",
    "docs = vectorizer.fit_transform(df['tweet'])\n",
    "\n",
    "print('end')\n",
    "#feature_names = vectorizer.get_feature_names()\n",
    "#visualizer = FreqDistVisualizer(features=feature_names, orient='v')\n",
    "#visualizer.fit(docs.toarray())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
